ğŸ§  Projet DEVA â€” Analyse Multimodale de Sentiment sur le Dataset MELD
ğŸ“Œ Introduction

Ce projet implÃ©mente un modÃ¨le dâ€™analyse de sentiment multimodal, inspirÃ© de lâ€™architecture DEVA, et appliquÃ© au dataset MELD (Multimodal EmotionLines Dataset).
Lâ€™objectif est de prÃ©dire le sentiment dâ€™une utterance (positif, nÃ©gatif, neutre) dans une conversation, en combinant trois sources dâ€™information :

Texte

Audio

VidÃ©o (expressions faciales)

Lâ€™approche repose sur une idÃ©e clÃ© :

ğŸ”„ Transformer les modalitÃ©s non textuelles en descriptions textuelles, puis les fusionner avec le texte original via attention croisÃ©e et une Minor Fusion Unit (MFU).

âœ¨ FonctionnalitÃ©s

ğŸ”Š Traitement Multimodal : fusion texte + audio + vision

ğŸ“ Encodage Textuel AvancÃ© : BERT + Transformer Encoder

ğŸ™ï¸ GÃ©nÃ©ration de Descriptions Audio et Visuelles

ğŸ”— MFU (Minor Fusion Unit) : fusion adaptative des modalitÃ©s via poids apprenables

ğŸ“Š Ã‰valuation complÃ¨te : Acc-2, F1, MAE, Pearson

âš–ï¸ Sampling StratifiÃ© : Ã©quilibrage des classes de sentiment

ğŸ—ï¸ Architecture du ModÃ¨le (DEVANet)
1. TextEncoder

Tokenisation via BertTokenizer

Embeddings contextuels via BertModel

Ajout dâ€™un token apprenable Em

Passage par un nn.TransformerEncoder

2. GÃ©nÃ©rateurs de Descriptions

AudioDescriptionGenerator
Convertit des features audio en phrases telles que :
"The speaker has high energy, low pitch, and fast speech."

VisionDescriptionGenerator
GÃ©nÃ¨re une description de lâ€™expression faciale :
"The person displays wide open eyes and raised eyebrows."

3. CrossModalAttention

MÃ©canisme nn.MultiheadAttention permettant au texte :

dâ€™interroger la description audio

dâ€™interroger la description visuelle

4. Minor Fusion Unit (MFU)

Module central du modÃ¨le :

Combine les informations provenant du texte, de lâ€™audio et de la vision

Poids apprenables :

Î± â†’ contribution de lâ€™audio

Î² â†’ contribution de la vision

5. PrÃ©dicteur

Un rÃ©seau feed-forward qui transforme lâ€™embedding fusionnÃ© en un score de sentiment continu entre -1.0 et 1.0.

ğŸ“š Dataset : MELD

Le dataset MELD contient des dialogues de la sÃ©rie Friends annotÃ©s avec :

Ã‰motions

Sentiments

Audio (formes dâ€™onde + MFCCs)

VidÃ©o (visages)

Le dataset est divisÃ© en train/val/test, avec sampling stratifiÃ© sur lâ€™entraÃ®nement et la validation.

âš™ï¸ Installation et Utilisation
ğŸ”§ PrÃ©requis
Python 3.x
torch
transformers
numpy
pandas
scikit-learn
scipy
tqdm
matplotlib
seaborn
pathlib

ğŸ“¥ TÃ©lÃ©chargement des donnÃ©es

Les donnÃ©es prÃ©traitÃ©es MELD peuvent Ãªtre obtenues via KaggleHub, ou via un chemin local similaire :

C:\Users\tmath\.cache\kagglehub\datasets\argish\meld-preprocessed\versions\1\preprocessed_data


Assurez-vous dâ€™adapter ce chemin selon votre environnement.

â–¶ï¸ ExÃ©cuter le Notebook
git clone <URL_DE_TON_DEPOT>
cd <ton_dossier_projet>


Installer les dÃ©pendances :

pip install -r requirements.txt
# ou manuellement
pip install torch transformers numpy pandas scikit-learn scipy tqdm matplotlib seaborn


Ouvrir le .ipynb dans Jupyter Notebook ou Google Colab et exÃ©cuter les cellules dans lâ€™ordre.

ğŸ“ˆ RÃ©sultats

AprÃ¨s 10 Ã©poques d'entraÃ®nement avec donnÃ©es samplifiÃ©es :

Performance (Test Set)
MÃ©trique	Valeur
Test Loss	0.0027
Acc-2	0.7467
F1-Score	0.7518
MAE	0.0390
Pearson	0.9983
ğŸ” Analyse des Poids du MFU
Poids	Valeur	Contribution
Î± (Audio)	0.9938	49.8%
Î² (Vision)	1.0033	50.2%

â¡ï¸ Le modÃ¨le utilise lÃ©gÃ¨rement plus la vision que lâ€™audio pour prÃ©dire le sentiment.

ğŸ”® Exemples de PrÃ©dictions

Le notebook inclut une section illustrant :

lâ€™utterance originale

les descriptions gÃ©nÃ©rÃ©es (audio + vision)

le score prÃ©dit

la comparaison avec le label rÃ©el
